{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4981fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Example usage:\n",
    "model_params = config['modeling']['classification_model']['params']\n",
    "features = config['features']['classification']\n",
    "model_path = config['models']['classification_model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf573a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Starting classification model development...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cca18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Load processed data\n",
    "df = pd.read_csv('../data/processed/classification_data.csv')\n",
    "print(f\"Classification data loaded. Shape: {df.shape}\")\n",
    "\n",
    "# Load feature info\n",
    "import json\n",
    "with open('../data/processed/feature_info.json', 'r') as f:\n",
    "    feature_info = json.load(f)\n",
    "\n",
    "classification_features = feature_info['classification_features']\n",
    "print(f\"Number of features: {len(classification_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ed8d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Prepare data for modeling\n",
    "print(\"=== PREPARING DATA FOR MODELING ===\")\n",
    "\n",
    "# Features and target\n",
    "X = df[classification_features]\n",
    "y = df['is_delayed']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf6364",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Model training - Random Forest\n",
    "print(\"=== TRAINING RANDOM FOREST MODEL ===\")\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074a301",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Model training - Logistic Regression\n",
    "print(\"=== TRAINING LOGISTIC REGRESSION MODEL ===\")\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Logistic Regression model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a60a0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Model evaluation\n",
    "print(\"=== MODEL EVALUATION ===\")\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"AUC-ROC:   {auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, 'precision': precision, 'recall': recall,\n",
    "        'f1': f1, 'auc': auc\n",
    "    }\n",
    "\n",
    "# Evaluate both models\n",
    "rf_metrics = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, \"Random Forest\")\n",
    "lr_metrics = evaluate_model(y_test, y_pred_lr, y_pred_proba_lr, \"Logistic Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5ccab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Confusion Matrix and ROC Curve\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Random Forest - Confusion Matrix')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "# Logistic Regression Confusion Matrix  \n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
    "axes[0,1].set_title('Logistic Regression - Confusion Matrix')\n",
    "axes[0,1].set_xlabel('Predicted')\n",
    "axes[0,1].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "\n",
    "axes[1,0].plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_metrics[\"auc\"]:.3f})')\n",
    "axes[1,0].plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {lr_metrics[\"auc\"]:.3f})')\n",
    "axes[1,0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[1,0].set_xlabel('False Positive Rate')\n",
    "axes[1,0].set_ylabel('True Positive Rate')\n",
    "axes[1,0].set_title('ROC Curves Comparison')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature Importance (Random Forest)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': classification_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "axes[1,1].barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "axes[1,1].set_yticks(range(len(feature_importance)))\n",
    "axes[1,1].set_yticklabels(feature_importance['feature'])\n",
    "axes[1,1].set_xlabel('Feature Importance')\n",
    "axes[1,1].set_title('Top 15 Feature Importances (Random Forest)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizations/model_results/classification_model_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef38460",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: SHAP Analysis\n",
    "print(\"=== SHAP ANALYSIS ===\")\n",
    "\n",
    "# Create SHAP explainer for Random Forest (better performing model)\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test.iloc[:1000])  # Use subset for speed\n",
    "\n",
    "# SHAP summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values[1], X_test.iloc[:1000], show=False)\n",
    "plt.title('SHAP Summary Plot - Classification Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizations/model_results/shap_summary_classification.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# SHAP feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values[1], X_test.iloc[:1000], plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance - Classification Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../visualizations/model_results/shap_importance_classification.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61103c86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9: OAI (Operational Adjustability Index) Calculation\n",
    "print(\"=== OPERATIONAL ADJUSTABILITY INDEX (OAI) ===\")\n",
    "\n",
    "def calculate_oai_classification(X, y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calculate OAI for classification model\n",
    "    OAI prioritizes controllable delays (carrier, late_aircraft)\n",
    "    \"\"\"\n",
    "    # Define controllable features (higher weights)\n",
    "    controllable_features = ['carrier_ct', 'late_aircraft_ct', 'controllable_delays']\n",
    "    controllable_weight = 2.0\n",
    "    \n",
    "    # Define uncontrollable features (lower weights)\n",
    "    uncontrollable_features = ['weather_ct', 'security_ct', 'uncontrollable_delays']\n",
    "    uncontrollable_weight = 0.5\n",
    "    \n",
    "    # Calculate weighted predictions\n",
    "    oai_scores = []\n",
    "    \n",
    "    for idx in range(len(X)):\n",
    "        row = X.iloc[idx]\n",
    "        base_pred = y_pred_proba[idx]\n",
    "        \n",
    "        # Calculate controllable factor\n",
    "        controllable_factor = 0\n",
    "        for feature in controllable_features:\n",
    "            if feature in X.columns:\n",
    "                controllable_factor += row[feature] * controllable_weight\n",
    "        \n",
    "        # Calculate uncontrollable factor\n",
    "        uncontrollable_factor = 0\n",
    "        for feature in uncontrollable_features:\n",
    "            if feature in X.columns:\n",
    "                uncontrollable_factor += row[feature] * uncontrollable_weight\n",
    "        \n",
    "        # OAI score emphasizes controllable delays\n",
    "        total_factor = controllable_factor + uncontrollable_factor\n",
    "        if total_factor > 0:\n",
    "            oai_score = base_pred * (controllable_factor / total_factor)\n",
    "        else:\n",
    "            oai_score = base_pred\n",
    "            \n",
    "        oai_scores.append(oai_score)\n",
    "    \n",
    "    return np.array(oai_scores)\n",
    "\n",
    "# Calculate OAI scores\n",
    "oai_scores = calculate_oai_classification(X_test, y_test, y_pred_proba_rf)\n",
    "\n",
    "# OAI-based AUC\n",
    "oai_auc = roc_auc_score(y_test, oai_scores)\n",
    "standard_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "print(f\"Standard AUC: {standard_auc:.4f}\")\n",
    "print(f\"OAI-weighted AUC: {oai_auc:.4f}\")\n",
    "print(f\"OAI focuses on controllable delays - difference: {oai_auc - standard_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f214",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: Model Selection and Saving\n",
    "print(\"=== MODEL SELECTION AND SAVING ===\")\n",
    "\n",
    "# Select best model based on metrics\n",
    "if rf_metrics['f1'] > lr_metrics['f1']:\n",
    "    best_model = rf_model\n",
    "    best_model_name = \"Random Forest\"\n",
    "    best_metrics = rf_metrics\n",
    "    best_predictions = y_pred_proba_rf\n",
    "else:\n",
    "    best_model = lr_model\n",
    "    best_model_name = \"Logistic Regression\"  \n",
    "    best_metrics = lr_metrics\n",
    "    best_predictions = y_pred_proba_lr\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "import os\n",
    "os.makedirs('../data/models/trained_models', exist_ok=True)\n",
    "\n",
    "joblib.dump(best_model, '../data/models/trained_models/classification_model.pkl')\n",
    "joblib.dump(explainer, '../data/models/trained_models/classification_explainer.pkl')\n",
    "\n",
    "# Save model performance metrics\n",
    "model_results = {\n",
    "    'model_type': 'classification',\n",
    "    'best_model': best_model_name,\n",
    "    'metrics': {\n",
    "        'random_forest': rf_metrics,\n",
    "        'logistic_regression': lr_metrics\n",
    "    },\n",
    "    'oai_metrics': {\n",
    "        'standard_auc': float(standard_auc),\n",
    "        'oai_auc': float(oai_auc)\n",
    "    },\n",
    "    'feature_count': len(classification_features),\n",
    "    'test_size': len(X_test)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/models/trained_models/classification_results.json', 'w') as f:\n",
    "    json.dump(model_results, f, indent=2)\n",
    "\n",
    "print(\"Classification model saved successfully!\")\n",
    "print(f\"Model file: classification_model.pkl\")\n",
    "print(f\"Results file: classification_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee052d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11: Final Summary\n",
    "print(\"=== CLASSIFICATION MODEL SUMMARY ===\")\n",
    "print(f\"✅ Model Type: Flight Delay Classification (Yes/No)\")\n",
    "print(f\"✅ Best Model: {best_model_name}\")\n",
    "print(f\"✅ Dataset Size: {len(df):,} records\")\n",
    "print(f\"✅ Features Used: {len(classification_features)}\")\n",
    "print(f\"✅ Test Set Performance:\")\n",
    "print(f\"   - Accuracy:  {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"   - Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"   - Recall:    {best_metrics['recall']:.4f}\")\n",
    "print(f\"   - F1-Score:  {best_metrics['f1']:.4f}\")\n",
    "print(f\"   - AUC-ROC:   {best_metrics['auc']:.4f}\")\n",
    "print(f\"✅ OAI Analysis: Completed (AUC: {oai_auc:.4f})\")\n",
    "print(f\"✅ SHAP Analysis: Completed\")\n",
    "print(f\"✅ Model Saved: ../data/models/trained_models/classification_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57e410",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
